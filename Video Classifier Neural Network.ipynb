{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ec344a7-d8f8-4f8d-a2bf-54622d641b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37f54cd1-57fd-4d8e-a55f-8e7b4362a644",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.listdir(\"DATASET/TRAIN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d275ac50-dd54-4e0a-bb7f-87d6ce427959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anomaly', 'Bad Footage', 'Empty', 'Full', 'Normal']\n"
     ]
    }
   ],
   "source": [
    "label_types = os.listdir(\"DATASET/TRAIN\")\n",
    "print(label_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d030ecc9-c5ec-4856-bec0-fa25aa62e142",
   "metadata": {},
   "source": [
    "# PREPARING TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e042212-dde7-4ba9-abb7-c0d4ad6ef90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       tag                                video_name\n",
      "0  Anomaly  DATASET/TRAIN/Anomaly/v001_converted.mp4\n",
      "1  Anomaly  DATASET/TRAIN/Anomaly/v002_converted.mp4\n",
      "2  Anomaly  DATASET/TRAIN/Anomaly/v003_converted.mp4\n",
      "3  Anomaly  DATASET/TRAIN/Anomaly/v004_converted.mp4\n",
      "4  Anomaly  DATASET/TRAIN/Anomaly/v006_converted.mp4\n",
      "        tag                                      video_name\n",
      "346  Normal  DATASET/TRAIN/Normal/chunk_Cam_3_Feb 22_25.mp4\n",
      "347  Normal  DATASET/TRAIN/Normal/chunk_Cam_3_Feb 22_26.mp4\n",
      "348  Normal   DATASET/TRAIN/Normal/chunk_Cam_3_Feb 22_6.mp4\n",
      "349  Normal   DATASET/TRAIN/Normal/chunk_Cam_3_Feb 22_7.mp4\n",
      "350  Normal   DATASET/TRAIN/Normal/chunk_Cam_3_Feb 22_8.mp4\n"
     ]
    }
   ],
   "source": [
    "data1= []\n",
    "\n",
    "for item in dataset_path:\n",
    "    # Getting file names\n",
    "    all_data = os.listdir(\"DATASET/TRAIN\" + \"/\"+item)\n",
    "\n",
    "    # Adding them in the list \n",
    "    for data in all_data:\n",
    "        data1.append((item,str(\"DATASET/TRAIN\" + \"/\"+item) + \"/\" + data))\n",
    "\n",
    "# Building dataframe \n",
    "train_df = pd.DataFrame(data = data1,columns = ['tag','video_name'])\n",
    "print(train_df.head())\n",
    "print(train_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e294ab2-1db9-4cd9-83c2-5ed2fbe2685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df.loc[:,['video_name','tag']]\n",
    "df\n",
    "df.to_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd02c09-d315-4ec4-b8a9-a79914f483c1",
   "metadata": {},
   "source": [
    "# PREPARING TEST DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "172334b1-8f5f-45d4-8693-c6b7b50466de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anomaly', 'Bad Footage', 'Empty', 'Full', 'Normal']\n"
     ]
    }
   ],
   "source": [
    "dataset_path = os.listdir('DATASET/TEST')\n",
    "print(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1af39c76-5a2c-4fcc-bcd6-dd89993438fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types of activities found : 5\n"
     ]
    }
   ],
   "source": [
    "label_types = os.listdir(\"DATASET/TEST\")\n",
    "print(\"Types of activities found :\",len(dataset_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecc6ad2d-96fe-4837-af8d-7b560a6480b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       tag                               video_name\n",
      "0  Anomaly  DATASET/TEST/Anomaly/v080_converted.mp4\n",
      "1  Anomaly  DATASET/TEST/Anomaly/v081_converted.mp4\n",
      "2  Anomaly  DATASET/TEST/Anomaly/v082_converted.mp4\n",
      "3  Anomaly  DATASET/TEST/Anomaly/v083_converted.mp4\n",
      "4  Anomaly  DATASET/TEST/Anomaly/v084_converted.mp4\n",
      "       tag                                     video_name\n",
      "85  Normal          DATASET/TEST/Normal/chunk_Cam3_17.mp4\n",
      "86  Normal          DATASET/TEST/Normal/chunk_Cam3_18.mp4\n",
      "87  Normal          DATASET/TEST/Normal/chunk_Cam3_19.mp4\n",
      "88  Normal  DATASET/TEST/Normal/chunk_Cam_3_Feb 22_27.mp4\n",
      "89  Normal  DATASET/TEST/Normal/chunk_Cam_3_Feb 22_28.mp4\n"
     ]
    }
   ],
   "source": [
    "data1= []\n",
    "\n",
    "for item in dataset_path:\n",
    "    # Getting file names\n",
    "    all_data = os.listdir(\"DATASET/TEST\" + \"/\"+item)\n",
    "\n",
    "    # Adding them in the list \n",
    "    for data in all_data:\n",
    "        data1.append((item,str(\"DATASET/TEST\" + \"/\"+item) + \"/\" + data))\n",
    "\n",
    "# Building dataframe \n",
    "test_df = pd.DataFrame(data = data1 ,columns = ['tag','video_name'])\n",
    "print(test_df.head())\n",
    "print(test_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20caad5d-6e04-435f-9435-09b68c3c8f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_df.loc[:,['video_name','tag']]\n",
    "df\n",
    "df.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0553acfd-618b-49ba-a3cd-d2453e5e0d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e15693c5-7317-49b0-986a-7cbf7b8f4eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow_docs.vis import embed\n",
    "import keras\n",
    "from imutils import paths\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53ae773c-b71f-475b-91fc-cc16ff91b338",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7656bd1-532c-4446-a281-6c4a090a98e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos for training: 351\n",
      "Total videos for testing: 90\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>video_name</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>106</td>\n",
       "      <td>DATASET/TRAIN/Bad Footage/chunk_Cam_3_Feb 11_8...</td>\n",
       "      <td>Bad Footage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>79</td>\n",
       "      <td>DATASET/TRAIN/Bad Footage/chunk_Cam_3_Feb 11_1...</td>\n",
       "      <td>Bad Footage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>270</td>\n",
       "      <td>DATASET/TRAIN/Normal/chunk_Cam_1_Feb 18_20.mp4</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>190</td>\n",
       "      <td>DATASET/TRAIN/Empty/chunk_Cam_2_Feb 18_10.mp4</td>\n",
       "      <td>Empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>256</td>\n",
       "      <td>DATASET/TRAIN/Normal/chunk_Cam_1_Feb 11_25.mp4</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>126</td>\n",
       "      <td>DATASET/TRAIN/Empty/chunk_Cam1_6.mp4</td>\n",
       "      <td>Empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>103</td>\n",
       "      <td>DATASET/TRAIN/Bad Footage/chunk_Cam_3_Feb 11_5...</td>\n",
       "      <td>Bad Footage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>328</td>\n",
       "      <td>DATASET/TRAIN/Normal/chunk_Cam_3_Feb 18_24.mp4</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>220</td>\n",
       "      <td>DATASET/TRAIN/Full/chunk_Cam_1_Feb 11_26.mp4</td>\n",
       "      <td>Full</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>149</td>\n",
       "      <td>DATASET/TRAIN/Empty/chunk_Cam2_8.mp4</td>\n",
       "      <td>Empty</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                         video_name  \\\n",
       "106         106  DATASET/TRAIN/Bad Footage/chunk_Cam_3_Feb 11_8...   \n",
       "79           79  DATASET/TRAIN/Bad Footage/chunk_Cam_3_Feb 11_1...   \n",
       "270         270     DATASET/TRAIN/Normal/chunk_Cam_1_Feb 18_20.mp4   \n",
       "190         190      DATASET/TRAIN/Empty/chunk_Cam_2_Feb 18_10.mp4   \n",
       "256         256     DATASET/TRAIN/Normal/chunk_Cam_1_Feb 11_25.mp4   \n",
       "126         126               DATASET/TRAIN/Empty/chunk_Cam1_6.mp4   \n",
       "103         103  DATASET/TRAIN/Bad Footage/chunk_Cam_3_Feb 11_5...   \n",
       "328         328     DATASET/TRAIN/Normal/chunk_Cam_3_Feb 18_24.mp4   \n",
       "220         220       DATASET/TRAIN/Full/chunk_Cam_1_Feb 11_26.mp4   \n",
       "149         149               DATASET/TRAIN/Empty/chunk_Cam2_8.mp4   \n",
       "\n",
       "             tag  \n",
       "106  Bad Footage  \n",
       "79   Bad Footage  \n",
       "270       Normal  \n",
       "190        Empty  \n",
       "256       Normal  \n",
       "126        Empty  \n",
       "103  Bad Footage  \n",
       "328       Normal  \n",
       "220         Full  \n",
       "149        Empty  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print(f\"Total videos for training: {len(train_df)}\")\n",
    "print(f\"Total videos for testing: {len(test_df)}\")\n",
    "\n",
    "\n",
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98b69a83-b3ab-45f1-9f5b-d6bf3ac993ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMG_SIZE = 224\n",
    "\n",
    "\n",
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d448bb-e83e-48d7-a22b-7792a1efbb34",
   "metadata": {},
   "source": [
    "# TO FEED THE VIDEO PERFORMING PROCESSING TASK :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5420de6-53ef-4db8-a394-4023b0ee5f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMG_SIZE = 224\n",
    "\n",
    "\n",
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9921fd-d0bc-4ce7-b87b-8a7efcab4e59",
   "metadata": {},
   "source": [
    "# FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1df75e1c-f6ce-4a47-b6df-63c589d0cf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06878e74-a49b-4275-83a4-925e90efdd59",
   "metadata": {},
   "source": [
    "# LABEL ENCODING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "35f7b872-726a-4554-b119-b3fc5c6279ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anomaly', 'Bad Footage', 'Empty', 'Full', 'Normal']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4]], dtype=int64)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]))\n",
    "print(label_processor.get_vocabulary())\n",
    "\n",
    "labels = train_df[\"tag\"].values\n",
    "labels = label_processor(labels[..., None]).numpy()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0b16b955-bafc-493c-9650-d3d210f30a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(351, 30, 2048)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_data[0].shape)\n",
    "train_data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a302212f-0452-46a4-b245-16949d09fc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define hyperparameters\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 100\n",
    "\n",
    "MAX_SEQ_LENGTH = 30\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "56621efb-ea35-43bf-a3c3-6c6f41e032aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame features in train set: (351, 30, 2048)\n",
      "Frame masks in train set: (351, 30)\n",
      "train_labels in train set: (351, 1)\n",
      "test_labels in train set: (90, 1)\n"
     ]
    }
   ],
   "source": [
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"video_name\"].values.tolist()\n",
    "    \n",
    "    ##take all classlabels from train_df column named 'tag' and store in labels\n",
    "    labels = df[\"tag\"].values\n",
    "    \n",
    "    #convert classlabels to label encoding\n",
    "    labels = label_processor(labels[..., None]).numpy()\n",
    "\n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\") # 145,20\n",
    "    frame_features = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\") #145,20,2048\n",
    "\n",
    "    # For each video.\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        # Gather all its frames and add a batch dimension.\n",
    "        frames = load_video(os.path.join(root_dir, path))\n",
    "        frames = frames[None, ...]\n",
    "\n",
    "        # Initialize placeholders to store the masks and features of the current video.\n",
    "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        # Extract features from the frames of the current video.\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                    batch[None, j, :]\n",
    "                )\n",
    "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "\n",
    "    return (frame_features, frame_masks), labels\n",
    "\n",
    "\n",
    "train_data, train_labels = prepare_all_videos(train_df, \"train\")\n",
    "test_data, test_labels = prepare_all_videos(test_df, \"test\")\n",
    "\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"train_labels in train set: {train_labels.shape}\")\n",
    "\n",
    "print(f\"test_labels in train set: {test_labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "71b67d0c-dabe-42e3-b3b4-90966907f229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
    "# `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
    "# masked with padding or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c511d7a-5eb0-4e78-b852-4bccac31e8ba",
   "metadata": {},
   "source": [
    "# SEQUENCE MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ddd3fbb5-7246-4fb6-8f76-b8ee18345c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.3027 - loss: 1.6086\n",
      "Epoch 1: val_loss improved from inf to 1.61577, saving model to ./model/video_classifier.weights.h5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 213ms/step - accuracy: 0.3116 - loss: 1.6083 - val_accuracy: 0.0000e+00 - val_loss: 1.6158\n",
      "Epoch 2/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.3615 - loss: 1.6046\n",
      "Epoch 2: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.3719 - loss: 1.6041 - val_accuracy: 0.0000e+00 - val_loss: 1.6227\n",
      "Epoch 3/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.3665 - loss: 1.6007\n",
      "Epoch 3: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.3757 - loss: 1.6000 - val_accuracy: 0.0000e+00 - val_loss: 1.6298\n",
      "Epoch 4/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.4095 - loss: 1.5938\n",
      "Epoch 4: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.4092 - loss: 1.5937 - val_accuracy: 0.0000e+00 - val_loss: 1.6367\n",
      "Epoch 5/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.4249 - loss: 1.5897\n",
      "Epoch 5: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.4211 - loss: 1.5895 - val_accuracy: 0.0000e+00 - val_loss: 1.6436\n",
      "Epoch 6/50\n",
      "\u001b[1m6/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.4184 - loss: 1.5866\n",
      "Epoch 6: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.4143 - loss: 1.5859 - val_accuracy: 0.0000e+00 - val_loss: 1.6506\n",
      "Epoch 7/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.4089 - loss: 1.5794\n",
      "Epoch 7: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.4087 - loss: 1.5797 - val_accuracy: 0.0000e+00 - val_loss: 1.6576\n",
      "Epoch 8/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.3922 - loss: 1.5779\n",
      "Epoch 8: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.3958 - loss: 1.5773 - val_accuracy: 0.0000e+00 - val_loss: 1.6646\n",
      "Epoch 9/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.4017 - loss: 1.5731\n",
      "Epoch 9: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.4025 - loss: 1.5729 - val_accuracy: 0.0000e+00 - val_loss: 1.6716\n",
      "Epoch 10/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3767 - loss: 1.5678\n",
      "Epoch 10: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3837 - loss: 1.5678 - val_accuracy: 0.0000e+00 - val_loss: 1.6786\n",
      "Epoch 11/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.3800 - loss: 1.5641\n",
      "Epoch 11: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.3863 - loss: 1.5640 - val_accuracy: 0.0000e+00 - val_loss: 1.6857\n",
      "Epoch 12/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.4346 - loss: 1.5585\n",
      "Epoch 12: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.4287 - loss: 1.5586 - val_accuracy: 0.0000e+00 - val_loss: 1.6927\n",
      "Epoch 13/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.4187 - loss: 1.5530\n",
      "Epoch 13: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.4175 - loss: 1.5533 - val_accuracy: 0.0000e+00 - val_loss: 1.6998\n",
      "Epoch 14/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3975 - loss: 1.5550\n",
      "Epoch 14: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.3999 - loss: 1.5545 - val_accuracy: 0.0000e+00 - val_loss: 1.7070\n",
      "Epoch 15/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.4304 - loss: 1.5482\n",
      "Epoch 15: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.4254 - loss: 1.5484 - val_accuracy: 0.0000e+00 - val_loss: 1.7141\n",
      "Epoch 16/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.3889 - loss: 1.5497\n",
      "Epoch 16: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.3932 - loss: 1.5485 - val_accuracy: 0.0000e+00 - val_loss: 1.7214\n",
      "Epoch 17/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.3909 - loss: 1.5416\n",
      "Epoch 17: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3928 - loss: 1.5415 - val_accuracy: 0.0000e+00 - val_loss: 1.7286\n",
      "Epoch 18/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.3805 - loss: 1.5407\n",
      "Epoch 18: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.3836 - loss: 1.5401 - val_accuracy: 0.0000e+00 - val_loss: 1.7359\n",
      "Epoch 19/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.4190 - loss: 1.5312\n",
      "Epoch 19: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.4178 - loss: 1.5316 - val_accuracy: 0.0000e+00 - val_loss: 1.7431\n",
      "Epoch 20/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3828 - loss: 1.5333\n",
      "Epoch 20: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3884 - loss: 1.5325 - val_accuracy: 0.0000e+00 - val_loss: 1.7504\n",
      "Epoch 21/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.3753 - loss: 1.5286\n",
      "Epoch 21: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.3826 - loss: 1.5281 - val_accuracy: 0.0000e+00 - val_loss: 1.7577\n",
      "Epoch 22/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.3606 - loss: 1.5416\n",
      "Epoch 22: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.3712 - loss: 1.5372 - val_accuracy: 0.0000e+00 - val_loss: 1.7650\n",
      "Epoch 23/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.4327 - loss: 1.5139\n",
      "Epoch 23: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.4272 - loss: 1.5158 - val_accuracy: 0.0000e+00 - val_loss: 1.7723\n",
      "Epoch 24/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.4119 - loss: 1.5224\n",
      "Epoch 24: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.4110 - loss: 1.5208 - val_accuracy: 0.0000e+00 - val_loss: 1.7796\n",
      "Epoch 25/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.3751 - loss: 1.5260\n",
      "Epoch 25: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.3825 - loss: 1.5231 - val_accuracy: 0.0000e+00 - val_loss: 1.7870\n",
      "Epoch 26/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.3587 - loss: 1.5239\n",
      "Epoch 26: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.3697 - loss: 1.5204 - val_accuracy: 0.0000e+00 - val_loss: 1.7944\n",
      "Epoch 27/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.4415 - loss: 1.4940\n",
      "Epoch 27: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.4341 - loss: 1.4973 - val_accuracy: 0.0000e+00 - val_loss: 1.8016\n",
      "Epoch 28/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.3752 - loss: 1.5125\n",
      "Epoch 28: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.3825 - loss: 1.5106 - val_accuracy: 0.0000e+00 - val_loss: 1.8090\n",
      "Epoch 29/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.3732 - loss: 1.4965\n",
      "Epoch 29: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.3810 - loss: 1.4975 - val_accuracy: 0.0000e+00 - val_loss: 1.8164\n",
      "Epoch 30/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.4328 - loss: 1.4954\n",
      "Epoch 30: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.4300 - loss: 1.4958 - val_accuracy: 0.0000e+00 - val_loss: 1.8236\n",
      "Epoch 31/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3895 - loss: 1.4988\n",
      "Epoch 31: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.3937 - loss: 1.4979 - val_accuracy: 0.0000e+00 - val_loss: 1.8310\n",
      "Epoch 32/50\n",
      "\u001b[1m6/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.3748 - loss: 1.5089\n",
      "Epoch 32: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.3852 - loss: 1.5034 - val_accuracy: 0.0000e+00 - val_loss: 1.8384\n",
      "Epoch 33/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.4593 - loss: 1.4746\n",
      "Epoch 33: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.4479 - loss: 1.4777 - val_accuracy: 0.0000e+00 - val_loss: 1.8457\n",
      "Epoch 34/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.4575 - loss: 1.4733\n",
      "Epoch 34: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.4465 - loss: 1.4766 - val_accuracy: 0.0000e+00 - val_loss: 1.8530\n",
      "Epoch 35/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.3666 - loss: 1.5019\n",
      "Epoch 35: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.3759 - loss: 1.4977 - val_accuracy: 0.0000e+00 - val_loss: 1.8604\n",
      "Epoch 36/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.3869 - loss: 1.4837\n",
      "Epoch 36: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.3916 - loss: 1.4829 - val_accuracy: 0.0000e+00 - val_loss: 1.8678\n",
      "Epoch 37/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.4282 - loss: 1.4741\n",
      "Epoch 37: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.4237 - loss: 1.4753 - val_accuracy: 0.0000e+00 - val_loss: 1.8752\n",
      "Epoch 38/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.4714 - loss: 1.4613\n",
      "Epoch 38: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.4573 - loss: 1.4649 - val_accuracy: 0.0000e+00 - val_loss: 1.8825\n",
      "Epoch 39/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3843 - loss: 1.4714\n",
      "Epoch 39: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3896 - loss: 1.4718 - val_accuracy: 0.0000e+00 - val_loss: 1.8899\n",
      "Epoch 40/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.4085 - loss: 1.4707\n",
      "Epoch 40: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.4084 - loss: 1.4711 - val_accuracy: 0.0000e+00 - val_loss: 1.8973\n",
      "Epoch 41/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3891 - loss: 1.4670\n",
      "Epoch 41: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.3933 - loss: 1.4674 - val_accuracy: 0.0000e+00 - val_loss: 1.9047\n",
      "Epoch 42/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.3801 - loss: 1.4811\n",
      "Epoch 42: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3864 - loss: 1.4778 - val_accuracy: 0.0000e+00 - val_loss: 1.9121\n",
      "Epoch 43/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.4410 - loss: 1.4557\n",
      "Epoch 43: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.4337 - loss: 1.4578 - val_accuracy: 0.0000e+00 - val_loss: 1.9194\n",
      "Epoch 44/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.4277 - loss: 1.4519\n",
      "Epoch 44: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.4234 - loss: 1.4543 - val_accuracy: 0.0000e+00 - val_loss: 1.9268\n",
      "Epoch 45/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3778 - loss: 1.4800\n",
      "Epoch 45: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.3846 - loss: 1.4757 - val_accuracy: 0.0000e+00 - val_loss: 1.9342\n",
      "Epoch 46/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.4087 - loss: 1.4660\n",
      "Epoch 46: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.4086 - loss: 1.4639 - val_accuracy: 0.0000e+00 - val_loss: 1.9416\n",
      "Epoch 47/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3965 - loss: 1.4453\n",
      "Epoch 47: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.3991 - loss: 1.4477 - val_accuracy: 0.0000e+00 - val_loss: 1.9490\n",
      "Epoch 48/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.4009 - loss: 1.4660\n",
      "Epoch 48: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.4025 - loss: 1.4625 - val_accuracy: 0.0000e+00 - val_loss: 1.9563\n",
      "Epoch 49/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.4160 - loss: 1.4456\n",
      "Epoch 49: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.4143 - loss: 1.4471 - val_accuracy: 0.0000e+00 - val_loss: 1.9636\n",
      "Epoch 50/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.4310 - loss: 1.4347\n",
      "Epoch 50: val_loss did not improve from 1.61577\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.4285 - loss: 1.4365 - val_accuracy: 0.0000e+00 - val_loss: 1.9710\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.2522 - loss: 1.6068\n",
      "Test accuracy: 27.78%\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "# Utility for running experiments.\n",
    "def run_experiment():\n",
    "    filepath = \"./model/video_classifier.weights.h5\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    "    )\n",
    "\n",
    "    seq_model = get_sequence_model()\n",
    "    history = seq_model.fit(\n",
    "        [train_data[0], train_data[1]],\n",
    "        train_labels,\n",
    "        validation_split=0.3,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint],\n",
    "    )\n",
    "\n",
    "    seq_model.load_weights(filepath)\n",
    "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100,2)}%\")\n",
    "\n",
    "    return history, seq_model\n",
    "\n",
    "\n",
    "_, sequence_model = run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2433657a-539b-4148-b765-62eb9df90983",
   "metadata": {},
   "source": [
    "# INFERENCE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2cf438bd-065b-432a-9ab6-686727d829a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test video path: DATASET/TEST/Normal/chunk_Cam2_11.mp4\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 993ms/step\n",
      "  Empty: 20.18%\n",
      "  Anomaly: 20.17%\n",
      "  Bad Footage: 19.90%\n",
      "  Full: 19.90%\n",
      "  Normal: 19.87%\n"
     ]
    }
   ],
   "source": [
    "def prepare_single_video(frames):\n",
    "    frames = frames[None, ...]\n",
    "    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    for i, batch in enumerate(frames):\n",
    "        video_length = batch.shape[0]\n",
    "        length = min(MAX_SEQ_LENGTH, video_length)\n",
    "        for j in range(length):\n",
    "            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
    "        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "    return frame_features, frame_mask\n",
    "\n",
    "\n",
    "def sequence_prediction(path):\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frames = load_video(os.path.join(\"test\", path))\n",
    "    frame_features, frame_mask = prepare_single_video(frames)\n",
    "    probabilities = sequence_model.predict([frame_features, frame_mask])[0]\n",
    "\n",
    "    for i in np.argsort(probabilities)[::-1]:\n",
    "        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
    "    return frames\n",
    "\n",
    "test_video = np.random.choice(test_df[\"video_name\"].values.tolist())\n",
    "print(f\"Test video path: {test_video}\")\n",
    "\n",
    "test_frames = sequence_prediction(test_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7787ed3-128d-42ef-86f6-920bdffd34a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <video alt=\"test\" width=\"520\" height=\"440\" controls>\n",
       "        <source src=\"DATASET/TEST/Anomaly/v098_converted.mp4\n",
       "        type=\"video/mp4\" style=\"height:300px;width:300px\">\n",
       "    </video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "    <video alt=\"test\" width=\"520\" height=\"440\" controls>\n",
    "        <source src=\"DATASET/TEST/Anomaly/v098_converted.mp4\n",
    "        type=\"video/mp4\" style=\"height:300px;width:300px\">\n",
    "    </video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5635d847-d0a2-40fc-8efd-862b10409d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
